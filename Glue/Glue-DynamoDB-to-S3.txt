
 Example 3: DynamoDB Tables Backup in S3 (Parquet)
 -------------------------------------------------
   
  URL: https://blog.clairvoyantsoft.com/aws-cloud-solution-dynamodb-tables-backup-in-s3-parquet-56466a4d921a

  For the end-to-end process, S3, Glue, DynamoDB, and Athena will be utilized and will follow these steps:

  1. Crawl the DynamoDB table with Glue to register the metadata of our table with the Glue Data Catalog. 
     Once thatâ€™s done, we can query the table with Athena.

  2. Create a Glue job for copying table contents into S3 in parquet format.

  3. Crawl the S3 bucket with Glue to register the bucket with the Glue Data Catalog and query it with Athena to verify the accuracy of the data copy.	


  Steps:

   1. Create an IAM Role with required Access

	Name: MyAWSGlueServiceRole
	Policies: 
		AmazonS3FullAccess
		CloudWatchFullAccess
 		AmazonDynamoDBFullAccess
 		AWSGlueServiceRole
 		AmazonRedshiftFullAccess

   2. Make sure that you have a DynamoDB table with data.  (table used here : users)

   3. Create an AWS Glue Crawler for DynamoDB

	-> Add information about your crawler
		-> Crawler name: ddb-backup-s3

	-> Specify crawler source type
		-> Crawler source type: Data stores
		-> Repeat crawls of S3 data stores: Crawl all folders

	-> Add a data store
		-> Choose a data store: DynamoDB
		-> Table name: users (you need to have this table in DynamoDB)
		-> Scanning rate: 0.5

	-> Add another data store: No

	-> Choose an IAM role
		Choose an existing IAM role: MyAWSGlueServiceRole

	-> Create a schedule for this crawler
		Frequency: Run on Demand

	-> Configure the crawler's output
		-> Database		
		   -> Add Database
			-> Database name: dynamodb-backup
			-> Create

	-> Review and click on 'Finish'

   4. Run Crawler
	-> Select the crawler and click on 'Run Crawler' button

	-> Once the crawler has finished running, navigate to the Tables under 
	   Data Catalog to ensure the table is created and the metadata looks 
           accurate such as the DynamoDB table schema and row count
		
   5. Create a Glue Job

	-> Configure the job properties
		-> Name: backup-dynamodb-on-s3
		-> IAM role: MyAWSGlueServiceRole
		-> Type: Spark
		-> Glue version: Spark 2.4, Python 3
		-> This job runs: A proposed script generated by AWS Glue
		-> Script file name: backup-dynamodb-on-s3

	-> Choose a data source
		-> Select DynamoDB table   (users - dynamodb-backup)

	-> Choose a transform type
		-> Change schema

	-> Choose a data target
		-> Create tables in your data target
		-> Data store: Amazon S3
		-> Format: Parquet
		-> Target path: s3://iquiz.glue/dynamodb-backup

	-> Output Schema Definition
		-> Review mappings and clikc on "Save Job and edit script"

        -> Review the code and click on "Run Job"
		
   
  6. Create a Glue Crawler for S3

	-> Add information about your crawler
		-> Crawler name: ddb-s3-view

	-> Specify crawler source type
		-> Crawler source type: Data stores
		-> Repeat crawls of S3 data stores : Crawl all folders

	-> Add a data store
		-> Choose a data store: S3
		-> Crawl data in: Specified path
		-> Include path: s3://iquiz.glue/dynamodb-backup

	-> Add another data store : No

	-> Choose an IAM role
		-> Choose an existing IAM role
			-> MyAWSGlueServiceRole

	-> Create a schedule for this crawler
		-> Frequency: Run on demand

	-> Configure the crawler's output
		-> Database: dynamodb-backup   (this is created in a prior step)

	-> Next
	-> Finish
		
  7. Run Crawler
	-> Select the crawler and click on 'Run Crawler' button		
				 

  8. Querying via Athena
	-> Navigate to AWS Athena and Connect to the Query editor 
	   (there should be a link on the home page)
	-> Select AWSDataCatalog for the Data Source on the left menu
	-> Choose the database in which the crawler will store the data.
	-> Use the query editor to run queries against the table created by crawler
	-> Test functionality by running the following command to ensure the format was extracted properly: 
		SELECT * FROM <db>.<table> LIMIT 10; to ensure data has been copied

	NOTE:
	Before you run your first query, you need to set up a query result location in Amazon S3.

	-> Settings -> manage -> Enter an S3 path where you want to store the results
