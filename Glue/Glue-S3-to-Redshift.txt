
 

   Moving Data from S3 to RedShift
   -------------------------------

    1. Create an IAM Role (MyAWSGlueServiceRole) for Glue with the following policies:

        Name: MyAWSGlueServiceRole			
	Service: Glue
	Policies:   
		-> AmazonS3FullAccess
		-> AmazonDynamoDBFullAccess
		-> AmazonRedshiftFullAccess
		-> AWSGlueServiceRole

    2. Create a Redshift Cluster
		
	-> Cluster identifier: 	glue-demo-cluster
	-> Free trial
	-> Admin user name:	gluedemoadmin
	-> Admin user password:	Gluedemoadmin123

    3. Edit the Security group

	-> Go to Network and security settings, and select the security group
	-> Add a rule to allow all traffic from Redshift.
	-> Add an additional 'self security group'
		-> Create a new security group (EC2 -> Security Groups) with an Inbound rule
		   to accept All TCP from itself
			Inbound Rules:
				Type: All TCP
				Source: Select Custom and add the 'same security group name' 
					like sg-035f20cefe8f6aa8a in the box.
	        -> Attach this to the VPC Security groups of the Redshift cluster
		-> Click on redshift cluster link you created
		-> Go to Properties Tab
		-> Click on 'Edit' button under 'Network and security settings' section.
		-> Under 'VPC security groups' section select this 'self' group and the default one. 

    4. Add VPC end point to S3
	
	-> Go to VPC service and click on 'End Points' link
	-> Select com.amazonaws.us-east-1.s3  (amazon - Gateway)
		-> You may have to browse to page 3 of the list.
	-> Select the default route table  (ex: rtb-060dc26feeaa6ddde)
	-> Click on 'Create Endpoint' button
				
    5. Open the Redshift cluster and create a table
	-> Click on redshift cluster
	-> Open Editor -> Query Editor
	-> Connect to the database (ex:  database name: dev, database user: gluedemoadmin)
	-> run the following command:
		create table link (id integer primary key, url varchar(255) not null, name varchar(255) not null);
  
   6. Create a Glue connection
	-> Connect to Glue service and click on Connections
	-> Add Connection
		-> Connection name : gluedemocon
		-> Connection type : Amazon Redshift
		-> Next
		-> Cluster : glue-demo-cluster
		-> Database name: dev
		-> Username: gluedemoadmin
		-> Password: Gluedemoadmin123
		-> review and Finish
	-> Test the Connection
		-> Select the Connection and click 'Test Connection' button

    7. Create a Glue Crawler
	-> Crawler name : demo-redshift-crawler
	-> Crawler source type: Data stores
	-> Repeat crawls of S3 data stores : Crawl all folders
	-> Choose a data store: JDBC
	-> Connection: gluedemocon
	-> Include path: dev/public/link    (database: dev, schema: public, table: link)
	-> Add another data store: No
	-> IAM role: MyAWSGlueServiceRole
	-> Frequency: Run on demand
	-> Database:
		-> Add database: glue-redshift-db
	-> Review and finish

   8. Run the Crawler
	-> Select the crawler and click on 'Run Crawler' button.
	-> Wait until it is completed and in Stopping/Ready status

   9. Check the table 
	-> Click on Tables and check to see if the table is created. 

   10. Load data to be added to Redshift table to S3
	-> Data Loaded to : s3://iquiz.glue/links-s3-redshift/links.csv

   11. Create & Run another Glue crawler to crawl the above S3 data (created above)
	-> Crawler name: demo-s3-crawler
	-> Crawler source type: Data stores
	-> Repeat crawls of S3 data stores : Crawl all folders
	-> Choose a data store: S3
	-> Include path: s3://iquiz.glue/links-s3-redshift
	-> Add another data store: No
	-> IAM role: MyAWSGlueServiceRole
	-> Frequency: Run on demand
	-> Database:
		-> Add database: glue-redshift-db
	-> Review and finish	

	-> RUN THE CRAWLER

    12. Add a Job
	-> Name: glue-s3-redshift-job
	-> IAM role: MyAWSGlueServiceRole
	-> Security configuration, script libraries, and job parameters (optional)
		-> Number of workers: 2
		-> Job timeout (minutes): 10
	-> Keep all others as they are and click Next	
	-> Choose a data source: links_s3_redshift (classification: csv)
	-> Choose a transform type: Change schema
	-> Choose a data target: dev_public_link   (classification: redshift)
	-> Output Schema Definition: Check the mappings and click 'Save Job and edit script'

    13. Check the Job status and check the result
	-> Wait until the job is completed.
	-> Click on the Job to see its status.
	-> Go to Redshift Query Editor and query from the 'link' table to see the data being loaded there.



    

		


		
				 


