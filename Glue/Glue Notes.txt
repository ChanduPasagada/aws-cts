
 Ref URL: https://data.solita.fi/aws-glue-tutorial-with-spark-and-python-for-data-developers/

 Example 1: ETL from one S3 bucket to another S3 bucket
 ------------------------------------------------------

   1. Load the required data to S3
	1.1 Create two folders in s3 to read from and write to:
		s3://iquiz.glue/movies-s3-to-s3/read
		s3://iquiz.glue/movies-s3-to-s3/write
	1.2 Load the dataset to the read folder
		Upload the movies-data.csv file to s3://iquiz.glue/movies-s3-to-s3/read folder

   2. Create an IAM Role (MyAWSGlueServiceRole) for Glue with the following policies:			
	Service: Glue
	Policies:   
	  -> AmazonS3FullAccess
	  -> AmazonDynamoDBFullAccess
	  -> AmazonRedshiftFullAccess
	  -> AWSGlueServiceRole

   3. Connect to AWS Glue service and create a Glue crawler:
	3.1  In the left panel of the Glue management console click Crawlers.
	3.2  Click the blue Add crawler button.
	3.3  Give the crawler a name such as glue-s3-s3-crawler.
	3.4  In Add a data store menu choose S3 and select the bucket you created. 
		-> Drill down to select the folder : s3://iquiz.glue/movies-s3-to-s3/read
	3.5  In Choose an IAM role create new. 
		-> Name the role to for example glue-blog-tutorial-iam-role.
	3.6  In Configure the crawler’s output add a database called glue-s3-s3-db.

   4. Run the crawler
	4.1  Goto Crawlers link to list all crawlers
	4.2  Tick the crawler that you created and Click Run crawler.

		Notes:  
		------
		Once the data has been crawled, the crawler creates a metadata table from it. 
		You find the results from the Tables section of the Glue console. 
		The database that you created during the crawler setup is just an arbitrary way 
		of grouping the tables.

		Glue tables don’t contain the data but only the instructions how to access the data.

    5. Create the Glue job
	 5.1  Name the job as  glue-s3-s3-job.
	 5.2  Choose the same IAM role that you created for the crawler. 
	 5.3  Type: Spark.
	 5.4  Glue version: Spark 2.4, Python 3 (or whatever is the default)
	 	-> This job runs: A new script to be authored by you.
	 5.5  Security configuration, script libraries, and job parameters
		-> Worker type - Standard
	 	-> Number of workers: 2.  (This is the minimum and costs about 0.15$ per run)
	 	-> Job timeout: 10. Prevents the job to run longer than expected.
	 5.6  Choose a data source - select the source 'read'
	 5.7  Choose a transform type - Change schema
	 5.8  Choose a data target - 'read'
	 5.9  Click Next and then 'Save job and edit the script'.


    6. Provide the script for your ETL functionality
	  6.1 Write the script in the editor (E:\AWS\0-Material\Glue\scripts\glue-s3-to-s3.py)
	  6.2 Save the code in the editor - Click save button.
          6.3 click Run job.

    7. Check the results
	  7.1 Once the the job is complete the results of the script are stored in output s3 folder
	      -> Output s3 folder: s3://iquiz.glue/movies-s3-to-s3/write/



   Example 2: Moving Data from S3 to RedShift
   -------------------------------------------

    1. Create an IAM Role (MyAWSGlueServiceRole) for Glue with the following policies:

        Name: MyAWSGlueServiceRole			
	Service: Glue
	Policies:   
		-> AmazonS3FullAccess
		-> AmazonDynamoDBFullAccess
		-> AmazonRedshiftFullAccess
		-> AWSGlueServiceRole

    2. Create a Redshift Cluster
		
	-> Cluster identifier: 	glue-demo-cluster
	-> Free trial
	-> Admin user name:	gluedemoadmin
	-> Admin user password:	Gluedemoadmin123

    3. Edit the Security group

	-> Go to Network and security settings, and select the security group
	-> Add a rule to allow all traffic from Redshift.
	-> Add an additional 'self security group'
		-> Create a new security group (EC2 -> Security Groups) with an Inbound rule
		   to accept All TCP from itself
			Inbound Rules:
				Type: All TCP
				Source: Select Custom and add the 'same security group name' 
					like sg-035f20cefe8f6aa8a in the box.
	        -> Attach this to the VPC Security groups of the Redshift cluster
		-> Click on redshift cluster link you created
		-> Go to Properties Tab
		-> Click on 'Edit' button under 'Network and security settings' section.
		-> Under 'VPC security groups' section select this 'self' group and the default one. 

    4. Add VPC end point to S3
	
	-> Go to VPC service and click on 'End Points' link
	-> Select com.amazonaws.us-east-1.s3  (amazon - Gateway)
		-> You may have to browse to page 3 of the list.
	-> Select the default route table  (ex: rtb-060dc26feeaa6ddde)
	-> Click on 'Create Endpoint' button
				
    5. Open the Redshift cluster and create a table
	-> Click on redshift cluster
	-> Open Editor -> Query Editor
	-> Connect to the database (ex:  database name: dev, database user: gluedemoadmin)
	-> run the following command:
		create table link (id integer primary key, url varchar(255) not null, name varchar(255) not null);
  
   6. Create a Glue connection
	-> Connect to Glue service and click on Connections
	-> Add Connection
		-> Connection name : gluedemocon
		-> Connection type : Amazon Redshift
		-> Next
		-> Cluster : glue-demo-cluster
		-> Database name: dev
		-> Username: gluedemoadmin
		-> Password: Gluedemoadmin123
		-> review and Finish
	-> Test the Connection
		-> Select the Connection and click 'Test Connection' button

    7. Create a Glue Crawler
	-> Crawler name : demo-redshift-crawler
	-> Crawler source type: Data stores
	-> Repeat crawls of S3 data stores : Crawl all folders
	-> Choose a data store: JDBC
	-> Connection: gluedemocon
	-> Include path: dev/public/link    (database: dev, schema: public, table: link)
	-> Add another data store: No
	-> IAM role: MyAWSGlueServiceRole
	-> Frequency: Run on demand
	-> Database:
		-> Add database: glue-redshift-db
	-> Review and finish

   8. Run the Crawler
	-> Select the crawler and click on 'Run Crawler' button.
	-> Wait until it is completed and in Stopping/Ready status

   9. Check the table 
	-> Click on Tables and check to see if the table is created. 

   10. Load data to be added to Redshift table to S3
	-> Data Loaded to : s3://iquiz.glue/links-s3-redshift/links.csv

   11. Create & Run another Glue crawler to crawl the above S3 data (created above)
	-> Crawler name: demo-s3-crawler
	-> Crawler source type: Data stores
	-> Repeat crawls of S3 data stores : Crawl all folders
	-> Choose a data store: S3
	-> Include path: s3://iquiz.glue/links-s3-redshift
	-> Add another data store: No
	-> IAM role: MyAWSGlueServiceRole
	-> Frequency: Run on demand
	-> Database:
		-> Add database: glue-redshift-db
	-> Review and finish	

	-> RUN THE CRAWLER

    12. Add a Job
	-> Name: glue-s3-redshift-job
	-> IAM role: MyAWSGlueServiceRole
	-> Security configuration, script libraries, and job parameters (optional)
		-> Number of workers: 2
		-> Job timeout (minutes): 10
	-> Keep all others as they are and click Next	
	-> Choose a data source: links_s3_redshift (classification: csv)
	-> Choose a transform type: Change schema
	-> Choose a data target: dev_public_link   (classification: redshift)
	-> Output Schema Definition: Check the mappings and click 'Save Job and edit script'

    13. Check the Job status and check the result
	-> Wait until the job is completed.
	-> Click on the Job to see its status.
	-> Go to Redshift Query Editor and query from the 'link' table to see the data being loaded there.


 Example 3: DynamoDB Tables Backup in S3 (Parquet)
 -------------------------------------------------
   
  URL: https://blog.clairvoyantsoft.com/aws-cloud-solution-dynamodb-tables-backup-in-s3-parquet-56466a4d921a

  For the end-to-end process, S3, Glue, DynamoDB, and Athena will be utilized and will follow these steps:

  1. Crawl the DynamoDB table with Glue to register the metadata of our table with the Glue Data Catalog. 
     Once that’s done, we can query the table with Athena.

  2. Create a Glue job for copying table contents into S3 in parquet format.

  3. Crawl the S3 bucket with Glue to register the bucket with the Glue Data Catalog and query it with Athena to verify the accuracy of the data copy.	


  Steps:

   1. Create an IAM Role with required Access

	Name: MyAWSGlueServiceRole
	Policies: 
		AmazonS3FullAccess
		CloudWatchFullAccess
 		AmazonDynamoDBFullAccess
 		AWSGlueServiceRole
 		AmazonRedshiftFullAccess

   2. Make sure that you have a DynamoDB table with data.  (table used here : users)

   3. Create an AWS Glue Crawler for DynamoDB

	-> Add information about your crawler
		-> Crawler name: ddb-backup-s3

	-> Specify crawler source type
		-> Crawler source type: Data stores
		-> Repeat crawls of S3 data stores: Crawl all folders

	-> Add a data store
		-> Choose a data store: DynamoDB
		-> Table name: users (you need to have this table in DynamoDB)
		-> Scanning rate: 0.5

	-> Add another data store: No

	-> Choose an IAM role
		Choose an existing IAM role: MyAWSGlueServiceRole

	-> Create a schedule for this crawler
		Frequency: Run on Demand

	-> Configure the crawler's output
		-> Database		
		   -> Add Database
			-> Database name: dynamodb-backup
			-> Create

	-> Review and click on 'Finish'

   4. Run Crawler
	-> Select the crawler and click on 'Run Crawler' button

	-> Once the crawler has finished running, navigate to the Tables under 
	   Data Catalog to ensure the table is created and the metadata looks 
           accurate such as the DynamoDB table schema and row count
		
   5. Create a Glue Job

	-> Configure the job properties
		-> Name: backup-dynamodb-on-s3
		-> IAM role: MyAWSGlueServiceRole
		-> Type: Spark
		-> Glue version: Spark 2.4, Python 3
		-> This job runs: A proposed script generated by AWS Glue
		-> Script file name: backup-dynamodb-on-s3

	-> Choose a data source
		-> Select DynamoDB table   (users - dynamodb-backup)

	-> Choose a transform type
		-> Change schema

	-> Choose a data target
		-> Create tables in your data target
		-> Data store: Amazon S3
		-> Format: Parquet
		-> Target path: s3://iquiz.glue/dynamodb-backup

	-> Output Schema Definition
		-> Review mappings and clikc on "Save Job and edit script"

        -> Review the code and click on "Run Job"
		
   
  6. Create a Glue Crawler for S3

	-> Add information about your crawler
		-> Crawler name: ddb-s3-view

	-> Specify crawler source type
		-> Crawler source type: Data stores
		-> Repeat crawls of S3 data stores : Crawl all folders

	-> Add a data store
		-> Choose a data store: S3
		-> Crawl data in: Specified path
		-> Include path: s3://iquiz.glue/dynamodb-backup

	-> Add another data store : No

	-> Choose an IAM role
		-> Choose an existing IAM role
			-> MyAWSGlueServiceRole

	-> Create a schedule for this crawler
		-> Frequency: Run on demand

	-> Configure the crawler's output
		-> Database: dynamodb-backup   (this is created in a prior step)

	-> Next
	-> Finish
		
  7. Run Crawler
	-> Select the crawler and click on 'Run Crawler' button		
				 

  8. Querying via Athena
	-> Navigate to AWS Athena and Connect to the Query editor 
	   (there should be a link on the home page)
	-> Select AWSDataCatalog for the Data Source on the left menu
	-> Choose the database in which the crawler will store the data.
	-> Use the query editor to run queries against the table created by crawler
	-> Test functionality by running the following command to ensure the format was extracted properly: 
		SELECT * FROM <db>.<table> LIMIT 10; to ensure data has been copied

	NOTE:
	Before you run your first query, you need to set up a query result location in Amazon S3.

	-> Settings -> manage -> Enter an S3 path where you want to store the results
