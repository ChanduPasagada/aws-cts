
  Kinesis Streams
  ---------------

  1. Kinesis Data Streams store the data for 24 hours (retention period is configurable).
     Kinesis deletes the data after 24 hours.

	
  2. Consumers can not delete the data from Kinesis Streams (unlike SQS)
     The data is read-only for consumers.

  3. The delay between put and retrieve is less than 1 sec. (latency < 1 sec)

  4. Mulitple consumers can consume data concurrently.


   Shards
   ------
	1 shard =>  write:  1 MB/sec write  (1000 messages/sec)
		    read:   2 MB/sec read

	N shards => write:  N MB/sec write  (N*1000 messages/sec)
		    read:   2N MB/sec read

	1 Shard has a capacity of:
		=> 5 transactions/sec/ for reads
		=> Max. total data read rate of 2 MB/sec

		=> 1000 records/sec for writes
		=> Max. total data write rate of 1 MB/sec

	=> Total capacity of the stream is the sum of the capacities of all shards.


   Kinesis CLI Commands
   --------------------

    Ref URL:  https://docs.aws.amazon.com/cli/latest/reference/kinesis/index.html

	
    $ aws kinesis describe-stream --stream-name demo-stream --region us-east-1	
	=> The output shows lot of of details about the stream.


    $ aws kinesis put-record 
		--stream-name demo-stream 
		--partition-key 1 
		--data 1  
		--region us-east-1

    $aws kinesis put-record 
	--stream-name demo-stream 
	--partition-key 3 
	--data 3 
	--region us-east-1 
	--cli-binary-format raw-in-base64-out


    $ aws kinesis get-shard-iterator 
		--stream-name demo-stream 
		--shard-iterator-type TRIM_HORIZON
		--shard-id shardId-000000000000
		--region us-east-1

	NOTES: 	shard-id can be see from the describe command or when you  put a record. 
		shard-iterator-type TRIM_HORIZON reads data from the beginning

		** The above command returns shard-iterator, which you are going to need if
		   want to read from the stream.

     	
    $ aws kinesis get-records --shard-iterator xxxxxxxxxxxx --limit 2

	NOTES: 
	The above command returns a JSON with 2 records data and also the NextShardIterator
	which you have you to further iterate the shard.

	The output of the data is base64 encoded. You can decode the data using the following command.

	$echo MQ== | base64 --decode  (here MQ== is the base64 encoded data)
	

  3 APIs of Kinesis Scaling:

	-> SplitShard API
	-> MergeShards API
	-> UpdateShardCount API


     $ aws kinesis describe-stream 
	--stream-name demo-stream 
	--region us-east-1	
		
		=> This one shows us StartingHashKey and EndingHashKey. We can divide the EndingHashKey by
		   2 to create two equal Hash ranges.

   Split Shards
   ------------

     $ aws kinesis split-shard 
	--stream-name demo-stream 
	--shard-to-split shardId-000000000000 
	--new-starting-hash-key  xxxxxxxxxxxxxxxxx

	=> The above command splits the stream to created two new shards (created from the 
	   parent shard).

     $ aws kinesis describe-stream 
	--stream-name demo-stream 
	--region us-east-1

	=> Now write several messages to the stream with different partition-ids and observe
	   that they are written to different shards.
	
     $ aws kinesis put-record 
	--stream-name demo-stream 
	--partition-key 1 
	--data 1  
	--region us-east-1

     => Now you read both the shards by getting their corresponding shard-iterators.
        (use get-shard-iterator and get-records command described earlier).


   Merge Shards
   -------------

     $ aws kinesis merge-shards 
	--stream-name demo-stream 
	--shard-to-merge shardId-000000000001
	--adjacent-shard-to-merge shardId-000000000002

    $ aws kinesis describe-stream 
	--stream-name demo-stream 
	--region us-east-1


  Kinesis Agent
  -------------

	Ref URL: https://docs.aws.amazon.com/streams/latest/dev/writing-with-agents.html

	=> Kinesis agent can be used to write data to kinesis

	=> Launch an EC2 instance
	   -> Attach a Role that has "AWSKinesisFullAccess" 
	   -> Connect to the EC2 via SSH.

	Install "kinesis agent"
	
	  -> sudo yum install -y aws-kinesis-agent

	  -> sudo vi generatelogs.sh

		while(true) do
   		   sleep 10;
   		   echo `date +"%H:%M:%S"` "log text" >> /tmp/server.log
		done;

	  -> sudo chmod +x generatelogs.sh

	 -> ./generatelogs.sh &       
		-> note: & executes the script in the background.

	Configure Kinesis Agent
	-----------------------
	$ sudo vi /etc/aws-kinesis/agent.json

	    -> in this file, change the "filePattern" to "/tmp/server.log" and
	       "kinesisStream" to your stream name (ex: "LogStream")

	       "flows" : [
		   {
			"filePattern": "/tmp/server.log",
			"kinesisStream": "LogStream"
		
		   }
                   // remove the part after this .. (deliverystream details)

                ]

	Restart the agent:

	    $sudo service aws-kinesis-agent restart

	Check the logs:
	
	    $cd /tmp
	    $less server.log
	    $cd /var/log/aws-kinesis-agent
	    $ls
	    $less aws-kinesis-agent.log


  Kinesis Producer Library (KPL)
  ------------------------------
	URL: https://docs.aws.amazon.com/streams/latest/dev/developing-producers-with-kpl.html
	
	NOTE: Kinesis DataStreams API is different than Kinesis Producer Library

	=> KPL provides a layer of abstrcation for ingesting data into Kinesis streams.
	=> Can be used in Async and Synchnorous use-cases.

	=> KPL Key Concepts:
		URL: https://docs.aws.amazon.com/streams/latest/dev/kinesis-kpl-concepts.html
		-> Records
		-> Batching (Two Types of batching)
		     -> Aggregation
		     -> Collection

	=> Writing to your Kinesis Data Stream Using the KPL
		URL: https://docs.aws.amazon.com/streams/latest/dev/kinesis-kpl-writing.html



   Kinesis Client Library (KCL)
   -----------------------------
	URL: https://docs.aws.amazon.com/streams/latest/dev/building-consumers.html
	     https://docs.aws.amazon.com/streams/latest/dev/shared-throughput-kcl-consumers.html



   NOTE:  Kinesis Lambda functions need a timeout of minimum one minute. Keep this in mind while
          you are writing Lambda functions for Kinesis (for example for Firehose)





















======================================================================

Commands:


aws kinesis describe-stream --stream-name demo-stream --region us-east-1

aws kinesis put-record --stream-name demo-stream --partition-key 1 --data 1 --region us-east-1
aws kinesis put-record --stream-name demo-stream --partition-key 3 --data 3 --region us-east-1 --cli-binary-format raw-in-base64-out

aws kinesis get-shard-iterator --stream-name demo-stream --shard-iterator-type TRIM_HORIZON --shard-id shardId-000000000000

aws kinesis get-records --shard-iterator AAAAAAAAAAGY5CnuF5sG1kKhdL0H/aDUtekU6R1hOAt0hlZjgqPL6X8Dmfc8M/fkwKQfkJ7JYjo6XvBVBU5gPvDIoCBTC+T4ZdI1slfnnwLJIJFgNCi/MpILXYq4WMX7vhj6x8qO9wKUISe/eD3LeOPOzkWgrnk/1sTYZiCCvvgHJ+bfhBXclIxwOZcbwNA3kRjgYtbidvlwd+RuzhH0mZJ6X0Eb4LOQC3Yha+fOFoTAHSHzYkKwTg== --limit 2

aws kinesis split-shard --stream-name demo-stream --shard-to-split shardId-000000000000 --new-starting-hash-key  170141183460469231731687303715884105727

aws kinesis merge-shards --stream-name demo-stream --shard-to-merge shardId-000000000001 --adjacent-shard-to-merge shardId-000000000002	


Firehose
---------

aws firehose put-record --delivery-stream-name demo-firehose-stream --record Data=100 --cli-binary-format raw-in-base64-out

